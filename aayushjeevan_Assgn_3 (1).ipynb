{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_json_files(file_paths):\n",
    "    all_data = []\n",
    "    for path in file_paths:\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            if isinstance(data, list):\n",
    "                all_data.extend(data)\n",
    "            elif isinstance(data, dict):\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    # Extract the 'body' key from each entry, which contains the joke text\n",
    "    flattened_data = [entry.get('body', '') for entry in data if 'body' in entry]\n",
    "    \n",
    "    # Create a DataFrame with 'input_text' as the joke text\n",
    "    df = pd.DataFrame({'input_text': flattened_data})\n",
    "    \n",
    "    # Drop rows with missing 'input_text' and reset the index\n",
    "    df = df.dropna(subset=['input_text']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def create_splits(df, train_frac=0.8, val_frac=0.1):\n",
    "    train_size = int(len(df) * train_frac)\n",
    "    val_size = int(len(df) * val_frac)\n",
    "    train, val, test = np.split(df.sample(frac=1, random_state=52), [train_size, train_size + val_size])\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_word2vec(df, vector_size=256, window=5, min_count=2):\n",
    "    tokenized_text = [text.split() for text in df['input_text']]\n",
    "    w2v_model = Word2Vec(sentences=tokenized_text, vector_size=vector_size, window=window, min_count=min_count, sg=1)\n",
    "    return w2v_model\n",
    "\n",
    "def build_embedding_matrix(w2v_model, tokenizer, embed_dim):\n",
    "    vocab_size = len(tokenizer)\n",
    "    embedding_matrix = np.random.uniform(-0.01, 0.01, (vocab_size, embed_dim))\n",
    "    for word, idx in tokenizer.items():\n",
    "        if word in w2v_model.wv:\n",
    "            embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class JokeDatasetWithWord2Vec(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128, pad_token=\"<pad>\", eos_token=\"<eos>\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_token_id = tokenizer[pad_token]\n",
    "        self.eos_token_id = tokenizer[eos_token]\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded = [self.tokenizer.get(word, 2) for word in text.split()]\n",
    "        encoded = encoded[:self.max_length - 1] + [self.eos_token_id]\n",
    "        return encoded + [self.pad_token_id] * (self.max_length - len(encoded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['input_text']\n",
    "        encoded = self.encode(text)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoded[:-1]),\n",
    "            'labels': torch.tensor(encoded[1:])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "\n",
    "class JokeGeneratorWithWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, embedding_matrix, num_layers=4, num_heads=8, ff_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 128, embed_dim))\n",
    "        self.layers = nn.ModuleList([DecoderBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.size(1)\n",
    "        x = self.embedding(input_ids) + self.pos_embedding[:, :seq_len, :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_model_with_w2v(model, train_loader, val_loader, device, num_epochs=10, lr=3e-4):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss / len(train_loader)}\")\n",
    "\n",
    "        validate_model_with_w2v(model, val_loader, device, criterion)\n",
    "\n",
    "def validate_model_with_w2v(model, val_loader, device, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/11: 100%|██████████| 20835/20835 [36:01<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 7.214015742391885\n",
      "Validation Loss: 6.360261820648545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/11: 100%|██████████| 20835/20835 [36:06<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 6.099423872265908\n",
      "Validation Loss: 5.972874195699271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/11: 100%|██████████| 20835/20835 [36:10<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 5.8317500662683495\n",
      "Validation Loss: 5.878082489097873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/11: 100%|██████████| 20835/20835 [36:11<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 5.679184747297243\n",
      "Validation Loss: 5.776484123987794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/11: 100%|██████████| 20835/20835 [36:11<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 5.551323809599688\n",
      "Validation Loss: 5.743610115380754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/11: 100%|██████████| 20835/20835 [36:12<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 5.442734844129035\n",
      "Validation Loss: 5.725601047471938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/11: 100%|██████████| 20835/20835 [36:11<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 5.330412269525457\n",
      "Validation Loss: 5.726684239272193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/11: 100%|██████████| 20835/20835 [36:13<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 5.220041471011696\n",
      "Validation Loss: 5.7204284474854274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/11: 100%|██████████| 20835/20835 [36:12<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 5.1176265754315216\n",
      "Validation Loss: 5.739931574892861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/11: 100%|██████████| 20835/20835 [36:11<00:00,  9.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 5.032012863642081\n",
      "Validation Loss: 5.749354455933232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/11: 100%|██████████| 20835/20835 [36:11<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 4.9695179175438\n",
      "Validation Loss: 5.759552019525627\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load Data\n",
    "file_paths = [\"/home/aayushjeevan/Desktop/NLP/reddit_jokes.json\", \"/home/aayushjeevan/Desktop/NLP/stupidstuff.json\", \"/home/aayushjeevan/Desktop/NLP/wocka.json\"] \n",
    "raw_data = load_json_files(file_paths)\n",
    "\n",
    "# Prepare Dataset\n",
    "df = prepare_dataset(raw_data)\n",
    "\n",
    "# Train Word2Vec\n",
    "w2v_model = train_word2vec(df, vector_size=256)\n",
    "\n",
    "# Tokenizer and Embedding Matrix\n",
    "tokenizer = {word: idx for idx, word in enumerate(w2v_model.wv.index_to_key, start=4)}\n",
    "tokenizer.update({\"<pad>\": 0, \"<unk>\": 1, \"<eos>\": 2, \"<sos>\": 3})\n",
    "embedding_matrix = build_embedding_matrix(w2v_model, tokenizer, embed_dim=256)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train, val, test = create_splits(df)\n",
    "train_dataset = JokeDatasetWithWord2Vec(train, tokenizer)\n",
    "val_dataset = JokeDatasetWithWord2Vec(val, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = JokeGeneratorWithWord2Vec(\n",
    "    vocab_size=len(tokenizer), embed_dim=256, embedding_matrix=embedding_matrix)\n",
    "\n",
    "# Train and Validate\n",
    "train_model_with_w2v(model, train_loader, val_loader, device, num_epochs=11, lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke: the did why nazi\n"
     ]
    }
   ],
   "source": [
    "def generate_joke(model, tokenizer, input_text, max_length=128, temperature=1.0, device='cpu'):\n",
    "    # Tokenize the input text\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([tokenizer.get(word, tokenizer['<unk>']) for word in input_text.split()]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize the generated sequence with the input text\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    \n",
    "    # Generate tokens step by step\n",
    "    for _ in range(max_length - len(generated_ids)):\n",
    "        input_tensor = torch.tensor([generated_ids]).to(device)  # Input the current generated tokens\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)  # Get the logits for the next token\n",
    "        logits = logits[:, -1, :]  # Get the logits of the last token\n",
    "        probabilities = torch.nn.functional.softmax(logits / temperature, dim=-1)  # Apply softmax for probabilities\n",
    "        next_token_id = torch.multinomial(probabilities, 1).item()  # Sample the next token\n",
    "        \n",
    "        if next_token_id == tokenizer['<eos>']:  # Stop if the end of sequence token is generated\n",
    "            break\n",
    "        \n",
    "        generated_ids.append(next_token_id)  # Add the generated token to the sequence\n",
    "    \n",
    "    # Decode the generated token IDs back to text\n",
    "    generated_text = ' '.join([word for word, idx in tokenizer.items() if idx in generated_ids])\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "sample_input =  \"why did the nazi\"  \n",
    "\n",
    "generated_joke = generate_joke(model, tokenizer, sample_input, max_length=128, temperature=0.7, device=device)\n",
    "print(f\"Generated Joke: {generated_joke}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke: the a did give why chicken gun\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for the joke\n",
    "sample_input = \"why did the chicken\"         \n",
    "\n",
    "# Generate a joke\n",
    "generated_joke = generate_joke(model, tokenizer, sample_input, max_length=128, temperature=0.7, device=device)\n",
    "\n",
    "# Print the generated joke\n",
    "print(f\"Generated Joke: {generated_joke}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 5 samples from the test dataset:\n",
      "\n",
      "Sample 9043:\n",
      "Input Text: There's 2 grains of rice in the sink\n",
      "Generated Joke: the of in 2 There's sink rice grains\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 50300:\n",
      "Input Text: Reddit\n",
      "Generated Joke: Reddit\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 123918:\n",
      "Input Text: I would tell you but the joke would finish to fast.\n",
      "Generated Joke: the to I you but would tell joke finish fast.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 186644:\n",
      "Input Text: A man walks into a pub and sees a sign hanging over the bar which reads:\n",
      "\n",
      "Cheese Sandwich: $1.50\n",
      "Hand Job: $10.00\n",
      "\n",
      "He checks his wallet for the necessary payment, then he walks up to the bar and beckons to one of the exceptionally attractive blondes serving drinks to an eager-looking group of men.\n",
      "\n",
      "\"Yes?\" she inquires, with a knowing smile, \"can I help you?\"\n",
      "\n",
      "\"Yep, I was wondering,\" whispers the man, \"are you the one who gives the handjobs?\"\n",
      "\n",
      "\"Yes,\" she purrs, \"I am.\"\n",
      "\n",
      "Replies the man, \"Well, then, wash your hands because I want a cheese sandwich!\"\n",
      "Generated Joke: the and a to I of you he his was for with He man your up she A one an into \"I then over who want because walks which \"Well, bar sees man, help hands gives sign you?\" group drinks hanging then, knowing wash whispers pub blondes cheese attractive checks wallet \"Yes,\" men. \"can reads: smile, \"are serving am.\" necessary \"Yep, Hand Replies Cheese beckons \"Yes?\" exceptionally $10.00 inquires, sandwich!\" $1.50 payment, Sandwich: Job: handjobs?\" purrs, wondering,\" eager-looking\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 108436:\n",
      "Input Text: Apparently it just changes the color of the baby.\n",
      "Generated Joke: the of it just Apparently baby. color changes\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 167559:\n",
      "Input Text: Roses are red,\n",
      "Violets are blue.\n",
      "I copied your answers,\n",
      "and I failed too.\n",
      "\n",
      "Generated Joke: and I your are too. answers, failed red, copied Roses blue. Violets\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 194674:\n",
      "Input Text: A good friend will bail you out of jail.A great friend will be in the cell next to you saying,\"Damn, that was fun!\"\n",
      "Generated Joke: the to of you in was that A out be will next good friend great cell bail fun!\" <unk>\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 94370:\n",
      "Input Text: Thats your puke your eating\n",
      "Generated Joke: your eating Thats puke\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 153468:\n",
      "Input Text: In a dark and hazy room, peering into a crystal ball, the Mystic delivered grave news: \"There's no easy way to tell you this, so I'll just be blunt. Prepare yourself to be a widow. Your husband will die a violent and horrible death this year.\"\n",
      "\n",
      "Visibly shaken, the wife stared at the woman's lined face, then at the single flickering candle, then down at her hands.\n",
      "\n",
      "She took a few deep breaths to compose herself - and to stop her mind racing. She simply had to know.\n",
      "\n",
      "She met the Fortune Teller's gaze, steadied her voice and asked, \"Will I be acquitted?\" [Source](http://smile.xonaki.com/Joke/EN?categoryCode=EN&jokeId=569)\n",
      "Generated Joke: the and a to I you her at this be so just had into then will She down no - wife tell few way I'll husband In took stop asked, Your voice this, mind single room, met deep dark die know. death simply yourself herself easy hands. face, woman's horrible \"There's stared grave \"Will ball, delivered violent lined year.\" crystal shaken, news: compose peering breaths widow. Prepare flickering Visibly racing. candle, gaze, Fortune blunt. steadied Mystic hazy acquitted?\" <unk>\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 101884:\n",
      "Input Text: He's surprised when she slaps him across the face. \"What was that for?\" he asks. \n",
      "\n",
      "\"For 20 years of bad sex.\"\n",
      "\n",
      "The man then slaps his wife across the face. \"What was that for?\" she demands.\n",
      "\n",
      "\"For knowing the difference.\"\n",
      "\n",
      "Generated Joke: the of The he his was that man she him when then wife \"What years bad across He's asks. 20 face. surprised knowing for?\" \"For slaps sex.\" demands. difference.\"\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 50522:\n",
      "Input Text: \"Can't spell 'slaughter' without 'laughter'\"\n",
      "Generated Joke: without spell \"Can't <unk>\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 119965:\n",
      "Input Text: ...and I feel just awful for her. \n",
      "Generated Joke: I for just feel her. ...and awful\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1204:\n",
      "Input Text: It was just an alternative event.\n",
      "Generated Joke: was just an It alternative event.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 28203:\n",
      "Input Text: You only get blown if you're hot\n",
      "Generated Joke: in get if You only you're hot blown\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 95344:\n",
      "Input Text: A poo'et \n",
      "Generated Joke: A <unk>\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model on 5 samples from the test dataset\n",
    "def test_model_on_samples(model, tokenizer, test_dataset, device, num_samples=10, max_length=128, temperature=1.0):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Randomly sample `num_samples` examples from the test dataset\n",
    "    sampled_data = test_dataset.df.sample(n=num_samples, random_state=38)       \n",
    "    \n",
    "    print(\"Testing on 5 samples from the test dataset:\\n\")\n",
    "    \n",
    "    # Loop through the sampled data\n",
    "    for i, row in sampled_data.iterrows():\n",
    "        input_text = row['input_text']\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"Input Text: {input_text}\")\n",
    "        \n",
    "        # Generate a joke\n",
    "        generated_joke = generate_joke(model, tokenizer, input_text, max_length=max_length, temperature=temperature, device=device)\n",
    "        print(f\"Generated Joke: {generated_joke}\\n{'-' * 50}\\n\")\n",
    "\n",
    "# Call the test function with your model, tokenizer, and test dataset\n",
    "test_model_on_samples(model, tokenizer, JokeDatasetWithWord2Vec(test, tokenizer), device, num_samples=15, max_length=128, temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 FINETUNING USING GIVEN DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 02:41:52.974352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-18 02:41:52.981837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731877912.990993 2211617 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731877912.993658 2211617 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 02:41:53.003017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def load_and_preprocess_data(file_paths):\n",
    "    all_texts = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            jokes = json.load(f)\n",
    "            for joke in jokes:\n",
    "                # Combine title and body if both exist\n",
    "                text = \"\"\n",
    "                if 'title' in joke:\n",
    "                    text += joke['title'] + \"\\n\"\n",
    "                text += joke['body']\n",
    "                # Add special tokens to mark start and end of jokes\n",
    "                text = \"<|startoftext|>\" + text + \"<|endoftext|>\"\n",
    "                all_texts.append(text)\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "def train_model(model, train_dataloader, device, num_epochs=3):\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Create scheduler with warmup\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_dataloader) * num_epochs\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "def generate_joke(prompt, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/home/aayushjeevan/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/26044 [00:00<?, ?it/s]/tmp/ipykernel_2211617/2497498475.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Training: 100%|██████████| 26044/26044 [2:20:52<00:00,  3.08it/s, loss=2.44]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 26044/26044 [2:20:51<00:00,  3.08it/s, loss=0.673]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 26044/26044 [2:20:41<00:00,  3.09it/s, loss=2.2]    \n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model with some prompts:\n",
      "\n",
      "Prompt: What do you call a cow\n",
      "Generated: What do you call a cow with three legs?\n",
      "Ground beef.\n",
      "\n",
      "Prompt: A blonde woman\n",
      "Generated: A blonde woman got on a bus with her baby\n",
      "The bus driver says: \"That's the ugliest baby I've ever seen!\" \n",
      "\n",
      "The woman walks to the rear of the bus and sits down, fuming. She says to a man next to her: \"The driver just insulted me!\" The man says: \"You go up there and tell him off – go ahead, I'll hold your monkey for you.\"\n",
      "\n",
      "Prompt: At a dinner party\n",
      "Generated: At a dinner party for a group of friends\n",
      "A man is invited to a dinner party for a group of friends. As the host sets the table for the party, he notices a group of people at the table. He asks one of the people if they know anyone who knows anyone.\n",
      "\n",
      "The man says, \"I don't know. I don't know anyone.\"\n",
      "\n",
      "\"Well, I guess you could say I do.\"\n",
      "\n",
      "The host, intrigued, asks, \"So\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = {\"pad_token\": \"<|pad|>\", \"additional_special_tokens\": [\"<|startoftext|>\", \"<|endoftext|>\"]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    file_paths = ['/home/aayushjeevan/Desktop/nlp/reddit_jokes.json', '/home/aayushjeevan/Desktop/nlp/stupidstuff.json', '/home/aayushjeevan/Desktop/nlp/wocka.json']  # Replace with your actual file paths\n",
    "    texts = load_and_preprocess_data(file_paths)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = JokesDataset(texts, tokenizer)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_dataloader, device)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save_pretrained(\"finetuned_gpt2_jokes\")\n",
    "    tokenizer.save_pretrained(\"finetuned_gpt2_jokes\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_prompts = [\n",
    "        \"What do you call a cow\",\n",
    "        \"A blonde woman\",\n",
    "        \"At a dinner party\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting the model with some prompts:\")\n",
    "    for prompt in test_prompts:\n",
    "        generated_text = generate_joke(prompt, model, tokenizer, device)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Prompt: Why did the chicken cross the road\n",
      "Generated Joke: Why did the chicken cross the road?\n",
      "To get to the other side.\n",
      "\n",
      "Prompt: A programmer walks into a bar\n",
      "Generated Joke: A programmer walks into a bar...\n",
      "And says \"I'll have a H20.\"\n",
      "\n",
      "The bartender says \"Sorry, we don't serve alcohol.\"\n",
      "\n",
      "The programmer says \"Well, I'll have a H20 too.\"\n",
      "\n",
      "The bartender says \"Sorry, we don't serve alcohol.\"\n",
      "\n",
      "The programmer says \"Well, I'll have a H20 too.\"\n",
      "\n",
      "The bartender says \"Sorry, we don't serve alcohol.\"\n",
      "\n",
      "The programmer says \"Well,\n",
      "\n",
      "Prompt: Knock knock\n",
      "Generated Joke: Knock knock. Who's there?\n",
      "The pilot.\n",
      "\n",
      "The pilot who?\n",
      "\n",
      "The pilot who blew the plane.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_joke(prompt, model, tokenizer, device, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate a joke completion for the given prompt using the fine-tuned model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model_on_prompts(prompts, model_path=\"finetuned_gpt2_jokes\", max_length=100):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load fine-tuned model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    \n",
    "    # Test each prompt\n",
    "    for prompt in prompts:\n",
    "        generated_joke = generate_joke(prompt, model, tokenizer, device, max_length)\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print(f\"Generated Joke: {generated_joke}\")\n",
    "\n",
    "# List your test prompts here\n",
    "custom_prompts = [\n",
    "    \"Why did the chicken cross the road\",\n",
    "    \"A programmer walks into a bar\",\n",
    "    \"Knock knock\"\n",
    "]\n",
    "\n",
    "# Call the testing function\n",
    "test_model_on_prompts(custom_prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I conclude that my model after training on given dataset is not much successful in generating meaningful content whereas the GPT-2 model after finetuning on the same dataset  is giving excellent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 OUTPUT:\n",
    "Using device: cuda\n",
    "\n",
    "Prompt: There's 2 grains of rice in the sink\n",
    "Generated Joke: There's 2 grains of rice in the sink\n",
    "And a third is rice in the bath.\n",
    "\n",
    "Prompt: Reddit\n",
    "Generated Joke: RedditWhat's the difference between an epileptic oyster farmer and a hooker with diarrhea?\n",
    "One shucks between fits.\n",
    "\n",
    "Prompt: I would tell you but the joke would finish to fast.\n",
    "Generated Joke: I would tell you but the joke would finish to fast.\n",
    "So a man goes to a bar, and he sits down. A man next to him, a very attractive woman, leans in close and whispers something in his ear. The man gets up and goes to the bar. He orders a drink. He sits there for a while, and then hears a knock on the door. He opens it. A man standing nearby, in a very drunken state, says to the man, \"Exc\n",
    "\n",
    "Prompt: It was just an alternative event.\n",
    "Generated Joke: It was just an alternative event.\n",
    "I had a friend that had a dog, but he didn't like it.  He told me this joke about a dog that lived with two people.  One day he and his dog were walking down the street when they saw a man walking down the sidewalk.  He said to the dog, \"Hey man, what are you doing?\"  The dog replied, \"I'm walking down the street and I saw two people walking down the sidewalk.\" \n",
    "\n",
    "Prompt: Thats your puke your eating\n",
    "Generated Joke: Thats your puke your eating?\n",
    "I'm going to have to put my dick in the toilet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY DECODER MODEL OUTPUT :\n",
    "Input Text: There's 2 grains of rice in the sink\n",
    "Generated Joke: the of in 2 There's sink rice grains\n",
    "\n",
    "\n",
    "Input Text: Reddit\n",
    "Generated Joke: Reddit\n",
    "\n",
    "\n",
    "Input Text: I would tell you but the joke would finish to fast.\n",
    "Generated Joke: the to I you but would tell joke finish fast.\n",
    "\n",
    "Input Text: Thats your puke your eating\n",
    "Generated Joke: your eating Thats puke\n",
    "\n",
    "\n",
    "Input Text: It was just an alternative event.\n",
    "Generated Joke: was just an It alternative event.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be  clearly  seen that my decoder model is not giving much relevant output whereas finetuned GPT-2 gives quite good results \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
